{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, GlobalMaxPooling1D\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "data = pd.read_csv('Data/balenced_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function to clean and preprocess text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    # Tokenization\n",
    "    tokens = text.split()\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    # Join tokens back into a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "# Example usage\n",
    "# text = \"This is an example sentence.\"\n",
    "# processed_text = preprocess_text(text)\n",
    "# print(processed_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code will cause memory error: \n",
    "# Unable to allocate 45.7 GiB for an array with shape (26865, 228492) and data type float64\n",
    "\n",
    "# Drop rows with missing values in the 'text' column\n",
    "# data = data.dropna(subset=['text'])\n",
    "# Create a TF-IDF vectorizer\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# Fit and transform the preprocessed text data\n",
    "# features = vectorizer.fit_transform(data['text'])\n",
    "# Convert features to a dense matrix\n",
    "# features = features.todense()\n",
    "# Example usage\n",
    "# print(features.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the above mentioned memory problem, we manually limited the maximum features extracted. Limiting the number of features can potentially impact the model's performance, as it reduces the amount of information available for training.\n",
    "By setting a maximum number of features, we are essentially reducing the dimensionality of the feature space. This can help mitigate memory constraints and improve computational efficiency. However, it also means that some potentially relevant features may be discarded, which can result in a loss of information. It's important to strike a balance between reducing dimensionality for efficiency purposes and retaining enough informative features for effective model training. The optimal number of features may vary depending on the specific dataset and problem domain, so it's worth experimenting with different feature subset sizes to find the most suitable configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26865, 100000)\n"
     ]
    }
   ],
   "source": [
    "data_withoutna = data.dropna(subset=['text'])\n",
    "# Shuffle the data to get a random subset\n",
    "shuffled_data = shuffle(data_withoutna, random_state=42)\n",
    "# Set the maximum number of features and the sample size\n",
    "max_features = 100000\n",
    "sample_size = 26865\n",
    "# Create a TF-IDF vectorizer with limited features\n",
    "vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "# Sample a smaller subset of the data for feature extraction\n",
    "sampled_data = shuffled_data.sample(n=sample_size, random_state=42)\n",
    "# Fit and transform the preprocessed text data\n",
    "features = vectorizer.fit_transform(sampled_data['text'])\n",
    "# Example usage\n",
    "print(features.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21492, 100000) (21492,)\n",
      "(5373, 100000) (5373,)\n"
     ]
    }
   ],
   "source": [
    "# Split the features and labels into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data_withoutna['Source'], test_size=0.2, random_state=42)\n",
    "# Example usage\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinominalNB als Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    abstract       0.99      1.00      0.99       777\n",
      "     article       0.66      0.92      0.77       790\n",
      "        blog       0.86      0.22      0.35       811\n",
      "       movie       0.32      0.85      0.46       805\n",
      "      reddit       0.51      0.22      0.30       759\n",
      "        song       0.91      0.32      0.47       769\n",
      "     twitter       0.88      0.73      0.80       782\n",
      "\n",
      "    accuracy                           0.61      5493\n",
      "   macro avg       0.73      0.61      0.59      5493\n",
      "weighted avg       0.73      0.61      0.59      5493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "# Example usage\n",
    "# Make predictions on the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "# Evaluate the classifier\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    abstract       0.14      0.18      0.16       792\n",
      "     article       0.15      0.14      0.15       801\n",
      "        blog       0.14      0.18      0.16       775\n",
      "       movie       0.15      0.01      0.02       671\n",
      "      reddit       0.13      0.15      0.14       768\n",
      "        song       0.16      0.18      0.17       786\n",
      "     twitter       0.14      0.14      0.14       780\n",
      "\n",
      "    accuracy                           0.14      5373\n",
      "   macro avg       0.14      0.14      0.13      5373\n",
      "weighted avg       0.14      0.14      0.14      5373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an SVM classifier\n",
    "classifier = SVC()\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "# Evaluate the classifier\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    abstract       0.14      0.16      0.15       792\n",
      "     article       0.15      0.16      0.16       801\n",
      "        blog       0.14      0.15      0.14       775\n",
      "       movie       0.10      0.05      0.06       671\n",
      "      reddit       0.14      0.14      0.14       768\n",
      "        song       0.15      0.18      0.16       786\n",
      "     twitter       0.16      0.16      0.16       780\n",
      "\n",
      "    accuracy                           0.14      5373\n",
      "   macro avg       0.14      0.14      0.14      5373\n",
      "weighted avg       0.14      0.14      0.14      5373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest classifier\n",
    "classifier = RandomForestClassifier()\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "# Evaluate the classifier\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    abstract       0.15      0.13      0.14       792\n",
      "     article       0.14      0.09      0.11       801\n",
      "        blog       0.14      0.07      0.09       775\n",
      "       movie       0.14      0.04      0.07       671\n",
      "      reddit       0.14      0.50      0.22       768\n",
      "        song       0.16      0.10      0.12       786\n",
      "     twitter       0.17      0.08      0.11       780\n",
      "\n",
      "    accuracy                           0.14      5373\n",
      "   macro avg       0.15      0.14      0.12      5373\n",
      "weighted avg       0.15      0.14      0.12      5373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a Gradient Boosting classifier\n",
    "classifier = GradientBoostingClassifier()\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "# Evaluate the classifier\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9154214893941176\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    abstract       1.00      1.00      1.00       777\n",
      "     article       0.94      0.96      0.95       790\n",
      "        blog       0.91      0.85      0.88       811\n",
      "       movie       0.96      0.84      0.90       805\n",
      "      reddit       0.70      0.93      0.80       759\n",
      "        song       0.95      0.87      0.91       769\n",
      "     twitter       0.94      0.88      0.91       782\n",
      "\n",
      "    accuracy                           0.90      5493\n",
      "   macro avg       0.91      0.91      0.91      5493\n",
      "weighted avg       0.92      0.90      0.91      5493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Model selection and training\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Model evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'movie'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m num_classes \u001b[39m=\u001b[39m \u001b[39m7\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[39m# Convert the target variable to one-hot encoded format\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m y_train_encoded \u001b[39m=\u001b[39m to_categorical(y_train, num_classes)\n\u001b[0;32m     14\u001b[0m y_test_encoded \u001b[39m=\u001b[39m to_categorical(y_test, num_classes)\n\u001b[0;32m     16\u001b[0m \u001b[39m# Create a sequential model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\np_utils.py:64\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkeras.utils.to_categorical\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_categorical\u001b[39m(y, num_classes\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     25\u001b[0m     \u001b[39m\"\"\"Converts a class vector (integers) to binary class matrix.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[39m    E.g. for use with `categorical_crossentropy`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39m    [0. 0. 0. 0.]\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(y, dtype\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mint\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     65\u001b[0m     input_shape \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mshape\n\u001b[0;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m input_shape \u001b[39mand\u001b[39;00m input_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(input_shape) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\series.py:893\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m    847\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[39m    Return the values as a NumPy array.\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[39m          dtype='datetime64[ns]')\u001b[39;00m\n\u001b[0;32m    892\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 893\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'movie'"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Calculate the input dimension based on the number of features\n",
    "input_dim = X_train.shape[1]\n",
    "# Define the number of classes\n",
    "num_classes = 7\n",
    "\n",
    "# Convert the target variable to one-hot encoded format\n",
    "y_train_encoded = to_categorical(y_train, num_classes)\n",
    "y_test_encoded = to_categorical(y_test, num_classes)\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "# Add a dense layer with ReLU activation\n",
    "model.add(Dense(128, activation='relu', input_shape=(input_dim,)))\n",
    "# Add dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "# Add another dense layer with ReLU activation\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# Add dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "# Add the final dense layer with softmax activation for multi-class classification\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train_encoded, batch_size=32, epochs=10, validation_data=(X_test, y_test_encoded))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test_encoded, verbose=0)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensambling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m# Create a simple neural network model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model5 \u001b[39m=\u001b[39m Sequential()\n\u001b[1;32m----> 9\u001b[0m model5\u001b[39m.\u001b[39madd(Dense(\u001b[39m128\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, input_shape\u001b[39m=\u001b[39m(input_dim,)))\n\u001b[0;32m     10\u001b[0m model5\u001b[39m.\u001b[39madd(Dropout(\u001b[39m0.5\u001b[39m))\n\u001b[0;32m     11\u001b[0m model5\u001b[39m.\u001b[39madd(Dense(\u001b[39m64\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_dim' is not defined"
     ]
    }
   ],
   "source": [
    "# Create individual models\n",
    "model1 = MultinomialNB()\n",
    "model2 = RandomForestClassifier()\n",
    "model3 = GradientBoostingClassifier()\n",
    "model4 = SVC(probability=True)  # SVM model\n",
    "\n",
    "# Create a simple neural network model\n",
    "model5 = Sequential()\n",
    "model5.add(Dense(128, activation='relu', input_shape=(input_dim,)))\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(64, activation='relu'))\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(num_classes, activation='softmax'))\n",
    "model5.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Create a voting classifier\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('nb', model1),\n",
    "    ('rf', model2),\n",
    "    ('gb', model3),\n",
    "    ('svm', model4),\n",
    "    ('nn', model5)\n",
    "], voting='soft')\n",
    "\n",
    "# Train the voting classifier\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the voting classifier\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reddit']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess and extract features for new text\n",
    "new_text = \"This is a new text to classify.\"\n",
    "processed_new_text = preprocess_text(new_text)\n",
    "new_features = vectorizer.transform([processed_new_text])\n",
    "# Convert new_features to a dense matrix and then to a numpy array\n",
    "new_features_array = np.asarray(new_features.toarray())\n",
    "# Predict the source category for the new text\n",
    "predicted_category = best_classifier.predict(new_features_array)\n",
    "# Example usage\n",
    "print(predicted_category)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1231     abstract\n",
       "531      abstract\n",
       "20161        song\n",
       "6332      article\n",
       "11285        blog\n",
       "           ...   \n",
       "21575        song\n",
       "5390      article\n",
       "860      abstract\n",
       "15795      reddit\n",
       "23654     twitter\n",
       "Name: Source, Length: 21968, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing steps\n",
    "# Lowercase conversion\n",
    "data_test = data\n",
    "data_test['text'] = data_test['text'].str.lower()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(data['text'], data['Source'], test_size=0.2, random_state=42)\n",
    "y_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Feature engineering\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_new)\n",
    "y_test_encoded = label_encoder.transform(y_test_new)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and encoding using BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 16\n",
    "num_workers = 2\n",
    "max_length = 128\n",
    "\n",
    "# Define collate function\n",
    "def collate_fn(batch):\n",
    "    batch = [sample for sample in batch if sample is not None]\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "from text_classification_dataset import TextClassificationDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data_test['text'].tolist()\n",
    "labels = data_test['Source'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set the desired dataset size\n",
    "desired_dataset_size = 10000\n",
    "\n",
    "# Shuffle the texts and labels\n",
    "combined = list(zip(texts, labels))\n",
    "random.shuffle(combined)\n",
    "texts, labels = zip(*combined)\n",
    "\n",
    "# Select a subset of texts and labels\n",
    "texts = texts[:desired_dataset_size]\n",
    "labels = labels[:desired_dataset_size]\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = TextClassificationDataset(texts, labels, tokenizer, max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data loader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "total_loss = 0\n",
    "total_batches = 0\n",
    "\n",
    "for batch in train_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    total_loss += loss.item()\n",
    "    total_batches += 1\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "average_loss = total_loss / total_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels: ['abstract' 'article' 'blog' 'movie' 'reddit' 'song' 'twitter']\n"
     ]
    }
   ],
   "source": [
    "unique_labels = data['Source'].unique()\n",
    "print(\"Unique Labels:\", unique_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9150437007119847\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    abstract       1.00      1.00      1.00       777\n",
      "     article       0.95      0.96      0.95       790\n",
      "        blog       0.91      0.85      0.88       811\n",
      "       movie       0.95      0.84      0.89       805\n",
      "      reddit       0.71      0.93      0.80       759\n",
      "        song       0.94      0.88      0.91       769\n",
      "     twitter       0.94      0.88      0.91       782\n",
      "\n",
      "    accuracy                           0.90      5493\n",
      "   macro avg       0.91      0.91      0.91      5493\n",
      "weighted avg       0.92      0.90      0.91      5493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Handle missing values in text data\n",
    "data['text'].fillna(\"\", inplace=True)\n",
    "\n",
    "# Step 2: Feature engineering\n",
    "text_data = data['text'].tolist()\n",
    "target = data['Source']\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_text = [' '.join([stemmer.stem(word) for word in text.split()]) for text in text_data]\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(stemmed_text)\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Model selection and training\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Model evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
